*"Dado que el tejido del Universo es de la mayor perfecci√≥n y la obra del m√°s sabio Creador, nada en absoluto tiene lugar en el Universo sin que una regla de m√°ximo o m√≠nimo aparezca"* 
<p style="text-align: right">Leonard Euler</p>

# Marco te√≥rico

Un problema de optimizaci√≥n consiste en encontrar una asignaci√≥n de valores a un conjunto de variables de forma que cumplan un conjunto de restricciones y maximicen o minimicen una funci√≥n de costo. Estos se pueden clasificar de acuerdo con los los valores que pueden tener las variables que intervienen. Si los dominios de alguna de sus variables es el conjunto de los enteros estamos en un problema de optimizaci√≥n en enteros mixta. Dentro de la anterior categor√≠a, se dice que se trabaja en enteros puros si todas sus variables son de dominio entero. Un caso de especial en la anterior categor√≠a es cuando todas las variables son binarias (pueden adoptar solamente 0 o 1 como valores).

Estos problemas son fundamentales en diversas √°reas como log√≠stica, planificaci√≥n, asignaci√≥n de tareas y de forma general todas aquellos escenarios donde se disponen de con recursos limitados para resolver determinada situaci√≥n. La naturaleza combinatoria de estos problemas a menudo implica que el n√∫mero de soluciones posibles crezca exponencialmente con el tama√±o del problema, lo que potencia la necesidad de descubrir nuevas t√©cnicas y heur√≠sticas para mejorar la eficiencia los algoritmos exactos que garanticen soluciones √≥ptimas. Dicha eficiencia depende, en primer lugar, de c√≥mo se construyen los modelos y, en segundo lugar, de los m√©todos computacionales utilizados. 

Para esto podemos seguir dos paradigmas diferentes: la programaci√≥n entera y la programaci√≥n de satisfacci√≥n de restricciones

## Programaci√≥n en enteros

La programaci√≥n en enteros es un conjunto de herramientas ampliamente utilizadas para resolver el siguiente problema: Cual es el m√°ximo/m√≠nimo que alcanza la funci√≥n $c^Tx+d^Ty$ sujeto a las restricciones: $Ax‚â§p$, $By‚â§q$, $x‚â•0$, $y‚â•0$, $x‚àà \R^n$, $y‚àà \Z^m$.  

### Programaci√≥n lineal como base de la programaci√≥n en enteros

El paradigma antes mencionado es una extensi√≥n de la programaci√≥n lineal, ya que, de forma general, resolver un problema de programaci√≥n en enteros requiere resolver uno o varios problemas lineales.

El principal algoritmo utilizado para resolver un problema de optimizaci√≥n lineal es el m√©todo Simplex. Se centra en la resoluci√≥n de los modelos en su llamada forma est√°ndar: $min$ $c^Tx$ $s.a.$ $Ax=b$, $x‚â•0$. Es importante notar que todo modelo puede llevarse a forma est√°ndar siguiendo los siguientes pasos:
- Si el objetivo es maximizar $c^Tx$, este es equivalente a minimizar $-c^Tx$.
- Si hay una restricci√≥n de la forma $a^Tx‚â§b$, entonces se puede introducir una variable nueva $x^h=b-a^Tx$, de forma que la restricci√≥n se puede escribir como $a^Tx+x^h=b$. Homol√≥gamente, si existe una restricci√≥n de la forma $a^Tx‚â•b$ se puede redefinir como $a^Tx-x^h=b$. Las nuevas variables se suelen llamar variables de holguras.
- Si una variable $x_i$ es irrestricta en signo, esta se puede expresar como la resta de dos variables positivas $x_i^+$ y $x_i^-$.

Ejemplo:
$$max \text{ }z= -3x_1-x_2+x_3$$

$$s.a.$$

$$x_1+2x_2+x_3‚â•5$$

$$x_1-3x_2+5x_3‚â•-10$$

$$x_1,x_2‚â•0$$

Al estandarizar este problema obtenemos
$$min \text{ }z= 3x_1+x_2-x_3^++x_3^-$$

$$s.a.$$

$$x_1+2x_2+x_3^+-x_3^--x_1^h=5$$

$$-x_1+3x_2-5x_3^++5x_3^-+x_2^h=10$$

$$x_1,x_2,x_3^+,x_3^-,x_1^h,x_2^h‚â•0$$

Se dice que $x$ es soluci√≥n factible de un problema lineal en su forma est√°ndar si $Ax=b$. Si $x$ tiene un n√∫mero de componentes nulas menor o igual al n√∫mero de restricciones, se dice que $x$ es soluci√≥n factible b√°sica. De estas definiciones se deriva el Teorema Fundamental de la Programaci√≥n Lineal, el cual garantiza que si existe una soluci√≥n factible √≥ptima, entonces tambi√©n existe una soluci√≥n factible b√°sica √≥ptima. Este teorema es la base del m√©todo Simplex, que consiste en explorar √∫nicamente las soluciones factibles b√°sicas del problema en b√∫squeda del √≥ptimo.

El primer paso del algoritmo es encontrar una soluci√≥n factible b√°sica. Para eso se selecciona una cantidad de componentes de $x$ igual a la cantidad de restricciones. Sea $x_B$ las componentes elegidas y $x_r$ el resto. Entonces la matriz $A$ de la forma est√°ndar del problema se puede descomponer en dos matrices $B$ y $C$ de forma que:

$$Bx_B+Cx_r=b$$

Luego, se multiplican ambos miembros por $B^{-1}$, resultando:

$$x_B+Rx_r=y$$

Donde $R=B^{-1}C$ y $y=B^{-1}b$.

Esta es la llamada forma expl√≠cita del problema, donde a los componentes de $x_B$ se les conoce como componentes b√°sicos de la soluci√≥n. Y permite realizar la asignaci√≥n $x_B=y, x_r=0$. Garantizando una soluci√≥n b√°sica. 

En la forma expl√≠cita es importante asegurar que $y‚â•0$. Esto sumado a la dificultad computacional de calcular la inversa de una matriz, se suele usar el m√©todo Simplex de dos fases para la obtenci√≥n de la primera soluci√≥n factible b√°sica.

Una vez con la soluci√≥n factible b√°sica inicial, el algoritmo compara las evaluaciones entre la soluci√≥n actual y las adyacentes (aquellas que se obtienen al cambiar un componente b√°sico por uno no b√°sico). Si ninguna soluci√≥n adyacente tiene una evaluaci√≥n menor que la actual, entonces hemos encontrado una soluci√≥n √≥ptima, lo cual se asegura debido a la convexidad del conjunto de soluciones factibles. De lo contrario, se cambia de soluci√≥n factible b√°sica y se vuelve repite el proceso.

Aunque el m√©todo Simplex tiene un tiempo de ejecuci√≥n exponencial en el peor de los casos, en la pr√°ctica, es muy eficiente y r√°pido para la mayor√≠a de los problemas reales, siendo la herramienta por excelencia para su soluci√≥n. 

### Planos cortantes

En el siguiente problema de optimizaci√≥n:

$$max \text{ } 20x_1+10x_2+x_3$$

$$s.a.$$

$$3x_1+2x_2+10x_3=10$$

$$2x_1+4x_2+20x_3=15$$

$$x_1,x_2,x_3‚â•0$$

La soluci√≥n  de este problema es: $ùë•_1 = \frac54,ùë•_2 = \frac{25}{8},ùë•_3 = 0$. Sin embargo, si las variables involucradas fueran enteras, esta soluci√≥n no es factible. La soluci√≥n redondeada es: $ùë•_1 = 1,ùë•_2 = 3,ùë•_3 = 0$, con valor de la funci√≥n objetivo igual a 50. Sin embargo, la soluci√≥n $ùë•_1 = 2,ùë•_2 = 2,ùë•_3 = 0$, proporciona un valor de la funci√≥n objetivo igual a 60. Por otra parte, la soluci√≥n redondeada no satisface las restricciones del problema. Resulta por tanto de inter√©s dise√±ar algoritmos que manejen la condici√≥n de las variables de ser enteras.

Una forma de extender la programaci√≥n lineal a la programaci√≥n entera podr√≠a plantearse como encontrar la menor cobertura convexa que contiene todas las asignaciones satisfacibles del problema. Si $S$ es el conjunto de asignaciones reales posibles del problema, $conv(S)$ se denota como la menor cobertura convexa del mismo. Ejemplo:

$$max \text{ } 5x_1+6x_2$$

$$s.a.$$

$$10x_1+14x_2 ‚â§ 35$$

$$x_1,x_2‚â•0$$

$$x_1,x_2‚àà\Z$$

La menor cobertura convexa es:

$$conv(S)=\{(x_1,x_2)|x_1+x_2‚â§3 ‚àß x_1+2x_2‚â§4 ‚àß x_1,x_2‚â•0\}$$

En si hacemos Simplex al nuevo problema:

$$max \text{ } 5x_1+6x_2$$

$$s.a.$$

$$(x,y) ‚àà conv(S)$$

$$x_1,x_2‚àà\Z$$

Obtendremos la soluci√≥n $x_1=2, x_2=1$.

En la pr√°ctica, buscar la menor cobertura convexa es dif√≠cil e ineficiente. Sin embargo, es posible calcular algunas de las restricciones de una cobertura convexa que no descarte soluciones enteras y excluya soluciones reales a conveniencia. Se trata de trabajar con un conjunto convexo $Q$ tal que $conv(S)‚äÜQ‚äÜS$. Si en la asignaci√≥n √≥ptima de $Q$ las variables enteras poseen valores enteros, entonces esa es la soluci√≥n del problema. De lo contrario, se busca un nuevo conjunto convexo $Q'$ que no incluya dicha soluci√≥n, tal que $conv(S)‚äÜQ'‚äÜQ$. Este nuevo conjunto convexo se obtiene a partir de la introducci√≥n de una nueva restricci√≥n que no excluye variables reales. Y se repite el proceso.

Todos los procedimientos basados en la explicaci√≥n anteriormente planteada son conocidos como m√©todos de planos cortantes, y a las restricciones que se agregan se les denomina corte. Estos m√©todos comienzan con una relajaci√≥n inicial de las restricciones de n√∫meros enteros, lo que da como resultado una soluci√≥n fraccionaria. Posteriormente, se a√±aden iterativamente cortes para reforzar la relajaci√≥n hasta que se obtiene una soluci√≥n entera.

Existen varias t√©cnicas para generar planos de corte, pero la mayor√≠a se deriva del corte fundamental, y a partir de este se derivan los mas usados, como el corte de Gomory, el corte Primal Todo Entero y el corte  de Chv√°tal-Gomory.


### Ramificaci√≥n y acotaci√≥n

Sea un problema de optimizaci√≥n en enteros tal que, al resolver el problema relajado, una de sus variables enteras $x_i$ tenga un valor real $p$. Como su valor puede ser entero, se cumple que $x ‚â§ [p] ‚à® x ‚â• [p]+1$. Sabiendo esto, se pueden resolver 2 nuevos problemas de optimizaci√≥n, con cada uno con las restricciones del problema original adicionando las restricciones anteriores respectivamente a cada 1.  Finalmente, el √≥ptimo ser√° el menor (si se est√° minimizando; de lo contrario, ser√° el mayor) de los √≥ptimos de ambas ramas. El m√©todo descrito es conocido como Ramificaci√≥n y acotaci√≥n. 

Ve√°moslo en el siguiente ejemplo:

Maximizar
$$x_1 + x_2$$
sujeto a:
$$2x_1 + 2x_2 ‚â• 3$$

$$‚àí2x_1 + 2x_2 ‚â§ 3$$

$$4x_1 + 2x_2 ‚â§ 19$$

$$x_1, x_2 ‚â• 0$$

$$x_1, x_2 ‚àà \Z$$

Soluci√≥n √≥ptima: $x_1=2.67, x_2=4.16, Objective=6.83$

Entonces, el problema se divide en dos  subproblemas distintos: uno con la restricci√≥n extra $x_1‚â§2$ (Caso 1) y otro con la restricci√≥n extra $x_1‚â•3$ (Caso 2).

Para el caso 1 la soluci√≥n √≥ptima es: $x_1=2, x_2=3.5, Objective=5.5$

Para el caso 2 esta es: $x_1=2, x_2=3.5, Objective=6.5$.

En este caso se puede seguir ramificando por ambas vias. Espec√≠ficamente, si ramificamos el caso 2, este se dividir√≠a en el caso donde $x_2‚â•4$ y el caso donde $x_2‚â§3$.

Finalmente. Tras otras dos ramificaciones se puede llegar a que el √≥ptimo es $x1=3, x2=3, Objective=6$.

Ramificaci√≥n y acotaci√≥n permite explorar diferentes posibilidades de soluci√≥n dividiendo el problema en subproblemas m√°s manejables, mientras que los planos cortantes ayudan a eliminar soluciones no viables, mejorando la eficiencia del proceso de b√∫squeda. Ambas metodolog√≠as, aunque pueden ser aplicadas por separado, se complementan eficazmente en la b√∫squeda soluciones √≥ptimas. Dicha combinaci√≥n es conocida como *Branch and Cut*. 

### Uso de variables binarias para la modelaci√≥n de problemas

Es fundamental identificar qu√© problemas pueden ser representados como problemas de programaci√≥n entera. Para ello, resulta interesante explorar c√≥mo diversas restricciones de la l√≥gica de predicados pueden ser modeladas en este contexto. Al comprender esta relaci√≥n, podemos traducir enunciados l√≥gicos complejos en formulaciones matem√°ticas que se pueden resolver mediante t√©cnicas de optimizaci√≥n discreta, ampliando as√≠ el alcance de los problemas que podemos abordar.

Supongamos que queramos introducir las siguientes restricciones:

$$ ‚àë_j a_{ij}x_j ‚â§ b_i ‚üπ Œ¥_i = 1$$

$$  Œ¥_i = 1 ‚üπ ‚àë_j a_{ij}x_j ‚â§ b_i$$

Si estas restricciones se logran, se podr√≠a saber cuantas restricciones se cumplen en un modelo, haciendo bisecci√≥n entre una restricci√≥n y una variable binaria.

Para la primera f√≥rmula, al aplicar contrarrec√≠proco, se quiere que $‚àë_j a_{ij}x_j > b_i$, pero si $‚àÉ m:‚àë_j a_{ij}x_j‚â• m$ , entonces se puede crear la restricci√≥n $‚àë_j a_{ij}x_j ‚â• b_i+œµ+(m-b-œµ)Œ¥_i$. De esa forma, si $Œ¥_i=1$ es una restricci√≥n redundante, si $Œ¥_i=0$ entonces fuerza a incumplir la restricci√≥n objetivo.

Para la segunda, si $‚àÉ M:‚àë_j a_{ij}x_j‚â§ M$ entonces se puede introducir la restricci√≥n $‚àë_j a_{ij}x_j ‚â§ M-(M-b_i)Œ¥_i$. 

Luego, si no existieran dichos valores(m√°ximos o m√≠nimos alcanzables), entonces se dice que el problema no es MIP representable. Un ejemplo de esto seria: $x=0‚à® y=0$.

Una vez haciendo biyecci√≥n entre variables l√≥gicas y restricciones lineales, se pueden hacer operaciones l√≥gicas elementales:

$Œ¥_1 ‚à® Œ¥_2:Œ¥_1+Œ¥_2‚â•1$
$Œ¥_1 ‚àß Œ¥_2:Œ¥_1+Œ¥_2=2$
$¬¨Œ¥_1 :Œ¥_1=0$
$Œ¥_1 ‚üπ Œ¥_2:Œ¥_1‚â§ Œ¥_2$
$Œ¥_1 ‚ü∫ Œ¥_2:Œ¥_1=Œ¥_2$

De esta forma se podr√≠an modelar problemas escritos en formas normales conjuntivas y disyuntivas.


## Programaci√≥n de satisfacci√≥n de restricciones

Una forma de analizar un problema de optimizaci√≥n es como un problema de satisfacci√≥n de restricciones, que consiste en una tupla $(V,D,C)$ donde $V$ es un conjunto de variables, $D=\{D_v|v‚àà V\}$ es el conjunto de los conjuntos de los posibles valores que pueden tomar cada variable, y $C$ es un conjunto finito de restricciones de la forma $(R_i,S_i)$, con $S_i$ es un subconjunto ordenado de $V$ y $R_i$ es una relaci√≥n de tama√±o $|S_i|$. Una soluci√≥n es una asignaci√≥n a cada variable que pertenece a $V$ con uno de sus correspondientes valores en $D$ tal que se cumplan todas las restricciones en $C$.

La programaci√≥n de satisfacci√≥n de restricciones (CSP) es aquella especializada en resolver este tipo de problemas. Algunas de las operaciones (por ejemplo, la ramificaci√≥n) utilizadas son similares a las de IP, y tiene muchas caracter√≠sticas en com√∫n con los procedimientos de reducci√≥n que ahora se usan com√∫nmente para preprocesar modelos. Este enfoque no est√° concebido como un m√©todo de optimizaci√≥n propiamente, aunque se puede adaptar a √©l haciendo que el objetivo, con l√≠mites cada vez m√°s estrictos, sea una restricci√≥n.

### SAT como base de la satisfacci√≥n de restricciones

No siempre es obvio, con un solo problema, hasta qu√© punto se utiliza la l√≥gica o cuando se utilizan m√©todos m√°s anal√≠ticos. Sin embargo, hay una gran ventaja en poder moverse entre los dos y reconocer las relaciones entre ellos. En este sentido, la programaci√≥n entera y la l√≥gica son simbi√≥ticas.

En este contexto, el problema de satisfacibilidad booleana (SAT) emerge como un caso paradigm√°tico donde la l√≥gica y la optimizaci√≥n discreta se cruzan. El Teorema de Cook, propuesto por Stephen Cook en 1971, es un hito fundamental en la teor√≠a de la complejidad computacional, pues plantea que todo problema de la categor√≠a NP es reducible a SAT. Este consiste en determinar dado una f√≥rmula l√≥gica, si existe una interpretaci√≥n de la misma tal que esta sea verdadera. 

Todo lo anteriormente planteado permite resaltar la gran importancia que cobra la l√≥gica en este tipo de problemas, pues es la que permite deducir enunciados a partir de otros en funci√≥n de las reglas de deducci√≥n que lo conforman. M√°s espec√≠ficamente, la l√≥gica proposicional y la l√≥gica de predicados proporcionan un marco te√≥rico robusto para abordar los problemas SAT.

Como forma general, todo problema SAT se representa en su Forma Normal Conjuntiva(CNF) debido a las ventajas que √©sta posee, y todos los cuantificadores se sit√∫an al principio de la expresi√≥n (Prenex Normal Form). Es necesario se√±alar que toda expresi√≥n v√°lida de la l√≥gica de predicados puede llevarse a dicha forma.

Ejemplo:

$$‚àÄx(‚àÉy(Q(y)‚à®R(x))‚üπP(x))$$

Primero, eliminamos la implicaci√≥n utilizando la equivalencia $A‚ÜíB‚â°¬¨A‚à®B$:

$$‚àÄx(¬¨‚àÉy(Q(y)‚à®R(x)) ‚à® P(x))$$

A continuaci√≥n, aplicamos la equivalencia $¬¨‚àÉy(A)‚â°‚àÄx(¬¨A)$


$$‚àÄx(‚àÄy¬¨(Q(y)‚à®R(x)) ‚à® P(x))$$

Luego, movemos los cuantificadores hacia el exterior. Para esto √∫ltimo, aplicamos las reglas de distribuci√≥n de cuantificadores. En este caso, podemos mover el cuantificador universal hacia afuera:

$$‚àÄx‚àÄy(¬¨(Q(y)‚à®R(x))‚à® P(x))$$

Aplicamos la equivalencia: $¬¨(A‚à®B)‚â°¬¨A‚àß¬¨B$

$$‚àÄx‚àÄy((¬¨Q(y)‚àß¬¨R(x))‚à® P(x))$$

Finalmente, usamos la equivalencia $(A‚àßB)‚à®C‚â°(A‚à®C)‚àß(B‚à®C)$

$$‚àÄx‚àÄy((¬¨Q(y)‚à® P(x))‚àß(¬¨R(x)‚à® P(x)))$$

Cuando los dominios de las variables son finitos, entonces cualquier formula de la l√≥gica de predicados puede expresarse como una conjunci√≥n de clausulas de la l√≥gica proposicional. Esto es importante porque dicha l√≥gica es consistente y completa. Se dice que un sistema es consistente si no se pueden derivar contradicciones dentro de √©l, es decir, no se puede demostrar que un enunciado sea verdadero y falso simult√°neamente. Un sistema es completo si se puede deducir la veracidad o falsedad de cualquier enunciado que pueda ser formulado en el modelo del sistema. Sin embargo, estas dos propiedades no siempre pueden coexistir en todos los sistemas. Este dilema es especialmente relevante en el contexto de la teor√≠a de G√∂del, que establece que en cualquier sistema formal consistente que sea capaz de expresar la aritm√©tica b√°sica, incluye proposiciones que no pueden ser ni demostradas ni refutadas dentro del propio sistema. Lo cual significa que no puede ser completo.

A la hora de encarar un problema de optimizaci√≥n usando l√≥gica de predicados, es necesario a√±adir funciones, constantes y reglas que la involucren. Aunque la aritm√©tica completa sea no decidible, hay "teor√≠as" m√°s peque√±as dentro de ella que s√≠ lo son. Entre estas est√°n la aritm√©tica sin multiplicaci√≥n y la teor√≠a de orden lineal denso. Estas bastan para resolver cualquier modelo de optimizaci√≥n lineal.

Veamos lo anteriormente planteado en el siguiente ejemplo:

**Maximizar:** $$2x_1 + 3x_2 ‚àí x_3 $$

*subject to:*
$$ x_1 + x_2 ‚â§ 3 $$

$$ ‚àíx_1 + 2x_3 ‚â• ‚àí2 $$

$$ ‚àí2x_1 + x_2 ‚àí x_3 = 0 $$

$$ x_1, x_2, x_3 ‚àà \R $$

Esto planteado en l√≥gica de predicados ser√≠a:

$‚àÉ z,x_1,x_2,x_3 ($
- $z - 2x_1 - 3x_2 + x_3 = 0$ $‚àß$
- $ x_1 + x_2 ‚â§ 3 $ $‚àß$
- $ ‚àíx_1 + 2x_3 ‚â• ‚àí2 $ $‚àß$
- $ ‚àí2x_1 + x_2 ‚àí x_3 = 0 $ $‚àß$
- $ x_1‚â• 0 $ $‚àß$ 
- $ x_2‚â• 0 $ $‚àß$ 
- $ x_3‚â• 0 $ 

$)$

Luego, podr√≠amos despejar $x_3$ en la cuarta restricci√≥n y sustituir en el resto, eliminando asi una variable del problema.

$‚àÉ z,x_1,x_2($
- $z - 2x_1 - 3x_2 + (‚àí2x_1 + x_2) = 0$ $‚àß$
- $ x_1 + x_2 ‚â§ 3 $ $‚àß$
- $ ‚àíx_1 + 2(‚àí2x_1 + x_2) ‚â• ‚àí2 $ $‚àß$
- $ x_1‚â• 0 $ $‚àß$ 
- $ x_2‚â• 0 $ $‚àß$ 
- $ ‚àí2x_1 + x_2‚â• 0 $ 

$)$

De forma hom√≥loga, podr√≠amos despejar la variable $x_2$ en la primera restricci√≥n:

$‚àÉ z,x_1 ($
- $ x_1 + \frac z 2 -2x_1 ‚â§ 3 $ $‚àß$
- $ ‚àí5x_1 + 2(\frac z 2 -2x_1) ‚â• ‚àí2 $ $‚àß$
- $ x_1‚â• 0 $ $‚àß$ 
- $ \frac z 2 -2x_1 ‚â• 0 $ $‚àß$ 
- $ ‚àí2x_1 + \frac z 2 -2x_1 ‚â• 0 $

$)$

Luego despejemos la variable $x_1$ en todas las restricciones

$‚àÉ z (‚àÉ x_1 ($
- $ \frac z 2 - 3 ‚â§  x_1 $ $‚àß$
- $ \frac z 9 + \frac 2 9 ‚â• x_1 $ $‚àß$
- $ x_1‚â• 0 $ $‚àß$ 
- $ \frac z 4 ‚â• x_1 $ $‚àß$ 
- $ \frac z 8 ‚â• x_1 $ $))$

Notar que aqu√≠ deducimos que:

$‚àÉ z ($
- $ \frac z 2 - 3 ‚â§  \frac z 9 + \frac 2 9 $ $‚àß$
- $ \frac z 2 - 3 ‚â§ \frac z 4  $ $‚àß$
- $ \frac z 2 - 3 ‚â§ \frac z 8 $ $‚àß$
- $ 0 ‚â§  \frac z 9 + \frac 2 9 $ $‚àß$
- $ 0 ‚â§ \frac z 4  $ $‚àß$
- $ 0 ‚â§ \frac z 8 $ 

$)$

Concluyendo que $-2‚â§ z ‚â§ 8$. Y como el objetivo es maximizar. Se toma $z=8$. De aqu√≠ vemos que $0‚â§ x_1 ‚â§1$. Que tomando a $x_1=1$ nos queda que $x_2=2$ y $x_3 = 0$

Este procedimiento es conocido como m√©todo de eliminaci√≥n de cuantificadores, que si bien no es utilizado actualmente por existir soluciones mucho m√°s eficientes dentro de la programaci√≥n lineal como el m√©todo Simplex, te√≥ricamente demuestra que la programaci√≥n lineal es una teor√≠a decidible.

### Davis-Putnam

El algoritmo de Davis-Putnam(DP)  es un precursor de los algoritmos modernos para resolver SAT, el cual utiliza el principio de resoluci√≥n. Sea una instancia de SAT en CNF, sea $p$ una variable proposicional y sean $C_1=p ‚à® Q_1$  y  $C_2 = ¬¨p ‚à® Q_2$ cl√°usulas del problema, con $Q_1$ y $Q_2$ disyunciones de literales. Como $(p=1)‚üπ Q_2$ y $(p=0)‚üπ Q_1$ se puede deducir $Q_1‚à® Q_2$. Al aplicar iterativamente resoluci√≥n, podemos deducir posibles valores de variables o una contradicci√≥n. En este √∫ltimo caso, se dice que el problema es insatisfacible. 

Veamos el siguiente ejemplo: La siguiente formula sera satisfacible:

$$(a‚à® b) ‚àß(a‚à® ¬¨b) ‚àß (¬¨a‚à® c) ‚àß(¬¨a‚à® ¬¨c)$$

Al aplicar la regla de resoluci√≥n entre las primeras dos cl√°usulas obtenemos la nueva restricci√≥n $(a ‚à® a)$, la cual es l√≥gicamente equivalente a $(a)$. Si aplicamos nuevamente resoluci√≥n entre esta cl√°usula y las dos ultimas, deducimos $(c)$ y $(¬¨c)$. Si aplicamos resoluci√≥n somos capaces de ver que llegamos a un absurdo, por lo que la formula nunca ser√° satisfacible. 

El algoritmo de Davis-Putnam sirve como base para el desarrollo de todos los algoritmos utilizados para resolver el problema SAT, estableciendo un marco te√≥rico importante para la l√≥gica computacional.

### Davis-Logemann-Loveland

Por otra parte, Davis-Logemann-Loveland(DLL/DPLL) se centra en asignar iterativamente valores a las variables y deshaciendo dichas asignaciones en caso de conflicto. Este algoritmo refina Davis-Putnam e introduce t√©cnicas cruciales  como el backjumping y el aprendizaje de cl√°usulas. Se basa en tres hechos: 
1- Todo literal puro (se dice puro si el literal opuesto no esta presente) es asignado como cierto. Ejemplo: $(a‚à® b) ‚àß (a‚à® ¬¨c) ‚àß (d‚à® ¬¨c) ‚àß (¬¨d‚à® ¬¨b) ‚àß (b‚à® c) $. Aqu√≠ al no estar $¬¨a$ en ninguna cl√°usula, se puede asignar $a=1$ y reducir el problema a $(d‚à® ¬¨c) ‚àß (¬¨d‚à® ¬¨b) ‚àß (b‚à® c) $.
2- si una cl√°usula tiene todos sus literales negados excepto uno este ultimo debe ser cierto. Ejemplo $(a ‚à® b ‚à® c)‚àß(a‚à®¬¨b‚à®¬¨c)‚àß(¬¨a‚à® b‚à®¬¨c) ‚àß(¬¨a‚à® ¬¨b‚à® c).$ Si se hace la asignaci√≥n parcial $a=1, ¬¨c=1$, entonces la tercera clausula $(¬¨a‚à® ¬¨b‚à® c)$ solo puede cumplirse si $¬¨b=1$.
3- Si todos los literales de una cl√°usula est√°n negados, entonces la asignaci√≥n hecha hasta dicho punto es falsa.

El algoritmo tiene 5 etapas: 
1- Preprocesamiento: Aqu√≠ se buscan todos los literales puros y se les asigna valor 1.
2- ramificaci√≥n: Aqu√≠ se asigna valor a un literal. Una buena heur√≠stica a la hora de decidir que literal escoger es Variable State Independent Decaying Sum(VSIDS), que consiste en asignar un numero a cada literal, el cual empieza siendo la cantidad de cl√°usulas en las que aparece, se divide entre una constante (usualmente 2) peri√≥dicamente y se le suma 1 cada vez que aparece en una cl√°usula conflicto.
3- propagaci√≥n unitaria (llamado en ingles Unit Propagation), en esta etapa se asignan valores a aquellos literales cuyo valor se pueden deducir. Es una de las mejoras clave del DPLL sobre su predecesor
4- an√°lisis de conflicto: aqu√≠ se busca agregar restricciones adicionales basada en la asignaci√≥n parcial en caso de hallar una contradicci√≥n.
5- retroceso (com√∫nmente llamado  Backtracking), deshace asignaciones hechas en caso de darse una contradicci√≥n, para asi explorar nuevos casos.


**Ejemplo de SAT utilizando DPLL**

Consideremos la siguiente f√≥rmula en FNC:
$F=(A‚à®¬¨B)‚àß(B‚à®C)‚àß(¬¨A‚à®¬¨C)‚àß(¬¨B‚à®¬¨A)‚àß(D‚à®¬¨C)‚àß(¬¨A‚à®D)$

Paso 1: Preprocesamiento
Se buscan literales puros. En este caso, como $¬¨D$ no esta presente en ninguna cl√°usula, se asigna $D=true$, reduciendo $F$ a:
$$F=(A‚à®¬¨B)‚àß(B‚à®C)‚àß(¬¨A‚à®¬¨C)‚àß(¬¨B‚à®¬¨A)$$

Paso 2: Ramificaci√≥n
El algoritmo DPLL selecciona un literal para asignar un valor. Supongamos que elegimos $A$ y lo asignamos a verdadero:
$A=true$

Paso 3: Propagaci√≥n Unitaria
Despu√©s de asignar $A=true$, actualizamos la f√≥rmula. La cl√°usula $(A‚à®¬¨B)$ se satisface y se elimina por lo que $F$ se reduce a $F^‚Ä≤=(B‚à®C)‚àß(¬¨C)‚àß(¬¨B)$
Ahora, observamos las cl√°usulas $(¬¨C)$ y $(¬¨B)$. Esto implica que $C=false$ y $B=false$. Sin embargo, esto hace falsa la segunda cl√°usula.

Paso 4: An√°lisis del conflicto.
Debido a que la asignaci√≥n parcial conlleva a la cl√°usula vac√≠a, se puede adicionar una nueva cl√°usula $(¬¨A)$. Siendo ahora:
$$F=(A‚à®¬¨B)‚àß(B‚à®C)‚àß(¬¨A‚à®¬¨C)‚àß(¬¨B‚à®¬¨A)‚àß(¬¨A)$$

Paso 5: Retroceso
Se deshace la asignaci√≥n $A=true$.

Luego, al volver al paso 1 y ejecutar luego ejecutar el paso 2, se llega a que la asignaci√≥n:
$A=false, B=false,C=true, D=true$

Haciendo $F$ satisfacible.

### Consistencia como forma de propagaci√≥n de restricciones

La mayor√≠a de los algoritmos usados recaen en la propagaci√≥n de restricciones (constraint propagation) y se realiza mediante la comprobaci√≥n de consistencia entre los valores de las variables. Este proceso implica analizar las restricciones que vinculan diferentes variables y ajustar sus dominios en consecuencia, lo que implica eliminar aquellos que violen alguna restricci√≥n. Entre las formas de comprobar consistencia est√° la consistencia de nodo, que reduce el dominio de una variable a aquellos valores que cumplen con todas las restricciones unarias.

Tambi√©n se habla de la consistencia de arco, centrada en eliminar aquellos valores $a$ de una variable $x$ si no existen valores $b$ de una variable $y$ tales que $(a,b)$ satisfagan a todas las restricciones entre $x$ y $y$. Uno de los algoritmos m√°s utilizados para comprobar consistencia de arco es el algoritmo AC-3, el cual guarda todos los pares ordenados de variables en una cola. Luego saca iterativamente cada uno de estos pares $<x,y>$ hasta que la cola se quede vac√≠a, y comprueba la consistencia de arco para cada posible valor de $x$. Si un valor no cumple la consistencia de arcos, este valor es eliminado del dominio de $x$, y todos los pares de variables de la forma $<z,x>$ son reinsertados en la cola. El algoritmo tiene una complejidad de tiempo en el peor de los casos de $O(ed^3 )$, donde $e$ es la cantidad de pares y $d$ es el tama√±o de dominio m√°s grande. Tras aplicar la consistencia de arco, pueden surgir tres posibles escenarios: si todos los dominios de las variables quedan con exactamente 1 valor (en cuyo caso tenemos la asignaci√≥n satisfacible), si un dominio queda vac√≠o (en cuyo caso ocurrir√≠a una contradicci√≥n y se debe hacer backtrack en una asignaci√≥n) o si al menos un dominio queda con m√°s de un posible valor, en cuyo caso se le debe asignar un valor y volver a realizar consistencia de arco.

Otras formas de consistencia existentes son la consistencia de camino y la $k$-consistencia. La consistencia de camino considera no solo las restricciones binarias entre pares de variables, sino tambi√©n las relaciones a trav√©s de secuencias m√°s largas de variables. Aqu√≠, $u$ es un valor consistente de $x$ si para todo $y$ existe un $w$ tal que dado cualquier secuencia de variables $a_1, a_2, ... a_n$, con $a_1=x$ y $a_n=y$ tenga la secuencia de valores $v_1, v_2, ... v_n$ con $v_1=u$ y $v_n=w$ de forma que el par $<v_i,v_{i+1}>$ cumpla con todas las restricciones binarias entre $a_i$ y $a_{i+1}$, con $1‚â§ i ‚â§ n$. Si bien la aplicaci√≥n de la consistencia de camino garantiza un mayor nivel de consistencia que la consistencia del arco, todav√≠a no es suficiente para resolver CSP en general. Esto significa que garantizando dicha consistencia, no todas las asignaciones garantizadas por esta son necesariamente soluciones satisfacible. Por otra parte, la $k$-consistencia, se logra al garantizar que cualquier asignaci√≥n v√°lida de valores a $k-1$ variables garantiza la posibilidad de asignaci√≥n de un valor a otra cualquier otra variable. Se dice que se es fuertemente $k$-consistente si para todo $j<k$ se es $j$-consistente. Ambos tipos de consistencias son bastante costosos computacionalmente por lo que no es muy utilizado en la pr√°ctica en comparaci√≥n con la consistencia de arco.

Ahora, si se desea optimizar usando CSP, una forma de lograrlo es hacer b√∫squeda binaria sobre la funci√≥n objetivo. Sea $f(x)$ la funci√≥n objetivo a maximizar y sean $m$ y $M$ tales que $‚àÄ x:m‚â§ f(x)‚â§ M$. Esto permite hacer un problema de satisfacibilidad adicionando la restricci√≥n $f(x)‚â• \frac{M+m}2$. Si el problema es satisfacible con $f(x)=m'$ entonces se puede resolver el modelo nuevamente, pero esta vez con la restricci√≥n $f(x)‚â• \frac{M+m'}2$. En caso contrario se puede volver a realizar la b√∫squeda con la restricci√≥n $f(x)‚â• \frac{M'+m}2$ con $M'=\frac{M+m'}2$. El caso de parada es cuando $M=m$, haciendo que la respuesta final sea la ultima soluci√≥n encontrada.

### Restricciones globales de la programaci√≥n de satisfacci√≥n de restricciones

A diferencia de la programaci√≥n en enteros, que restringe su modelado a expresiones lineales, En la programaci√≥n por restricciones, los modelos suelen expresarse en forma de predicados, que si bien pudieran ser convertidos a modelos lineales, dicha conversi√≥n puede ser engorrosa. Dichos predicados suelen depender del software utilizado, y en muchos casos se da la oportunidad al usuario de definir predicados locales. Pero de forma general existen restricciones globales que suelen ser sem√°nticamente redundantes y permiten filtrar el dominio de las variables.

**Restricciones globales fundamentales:**

- **All Different**: Esta restricci√≥n fuerza a que todos los valores de las variables sean diferentes entre si.
- **Global Cardinality**: Estas restricciones controlan la cantidad de veces que ciertos valores pueden aparecer en un conjunto de variables. Por ejemplo, global_cardinality permite especificar cu√°ntas veces debe aparecer cada valor en un array de variables.
- **Inverse**: Esta restricci√≥n asegura que si un valor se asigna a una variable, entonces otro conjunto de variables debe reflejar esa asignaci√≥n en un orden inverso. Es √∫til para problemas donde la relaci√≥n entre las variables es crucial.
- **Table**: Permite definir restricciones basadas en una tabla predefinida que especifica combinaciones v√°lidas de valores para un conjunto de variables. Esto es √∫til para modelar relaciones complejas entre variables.
- **Circuit**: Asegura que un conjunto de variables forma un circuito, lo cual es esencial en problemas como el Traveling Salesman Problem. Esta restricci√≥n garantiza que no haya subcircuitos y que todos los nodos sean visitados.
- **Lexicographic Order (Lex)**: Se utiliza para imponer un orden lexicogr√°fico entre dos o m√°s secuencias de variables, lo que puede ser √∫til en problemas donde el orden relativo es importante.
- **Element**: Esta restricci√≥n permite acceder a los elementos de un array mediante √≠ndices definidos por otras variables, facilitando la modelizaci√≥n de problemas donde se necesita seleccionar entre m√∫ltiples opciones.
- **Cumulative**: Se utiliza para gestionar recursos limitados en el tiempo, asegurando que las demandas no excedan la capacidad disponible en cada momento.
- **Regular**: Permite definir restricciones sobre cadenas de longitud variable y es √∫til en problemas relacionados con aut√≥matas y gram√°ticas formales.